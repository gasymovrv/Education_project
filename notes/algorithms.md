# Алгоритмы

## Оценка сложности алгоритмов O(f(n))

### Что такое O(f(n))
+ O(1) - затраты времени не зависят от размера задачи
+ O(log(n)) - при увеличении размера задачи вдвое, затраты времени меняются на постоянную величину
    + log~a~(n) = x, где a - основание, n - результат возведения в степень, x - степень основания
    + Основание нам не важно, главное порядок изменения сложности при увеличении объема входных данных (n)
+ O(n) - при увеличении размера задачи в 2 раза, затраты времени возрастут тоже в два раза
+ O(n^2) - при увеличении размера задачи в 2 раза, затраты времени возрастут примерно в четыре раза
+ O(n\*log(n)) - при увеличении задачи в два раза, затраты времени возрастут в два раза, плюс некоторая прибавка, относительный вклад которой уменьшается с ростом n. При малых n может вносить очень большой вклад. O(n*log(n)) начинает расти как квадрат при малых n, но потом рост замедляется почти до линейного

### Примеры
+ O(n) — линейная сложность
Такой сложностью обладает, например, алгоритм поиска наибольшего элемента в не отсортированном массиве. Нам придётся пройтись по всем n элементам массива, чтобы понять, какой из них максимальный.

+ O(log n) — логарифмическая сложность
Простейший пример — бинарный поиск. Если массив отсортирован, мы можем проверить, есть ли в нём какое-то конкретное значение, методом деления пополам. Проверим средний элемент, если он больше искомого, то отбросим вторую половину массива — там его точно нет. Если же меньше, то наоборот — отбросим начальную половину. И так будем продолжать делить пополам, в итоге проверим log n элементов.

+ O(n^2) — квадратичная сложность
Такую сложность имеет, например, алгоритм сортировки вставками. В канонической реализации он представляет из себя два вложенных цикла: один, чтобы проходить по всему массиву, а второй, чтобы находить место очередному элементу в уже отсортированной части. Таким образом, количество операций будет зависеть от размера массива как n * n, т. е. n2.
