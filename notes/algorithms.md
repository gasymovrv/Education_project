# Алгоритмы

## Оценка сложности алгоритмов O(f(n))

### Что такое O(f(n))
+ O(1) - затраты времени не зависят от размера задачи
+ O(log(n)) - при увеличении размера задачи вдвое, затраты времени меняются на постоянную величину
    + log~a~(n) = x, где a - основание, n - результат возведения в степень, x - степень основания
        + Бывают ln (ln(n)= log~e~(n), e = 2,71828...) и lg (lg(n)= log~10~(n))
    + Основание нам не важно, главное порядок изменения сложности при увеличении объема входных данных (n)
    + пример (в обоих случаях переход от n = 10 к 100 вызывает изменение функции (увеличивает время работы алгоритма) в 20 раз, от 1000 к 10000 - в 13.3 раза): 

| n            | 10 | 100 | 1000 | 10000 |
|--------------|----|-----|------|-------|
| n*lg(n)      | 10 | 200 | 3000 | 40000 |
| n*ln(n)      | 23 | 460 | 6900 | 92000 |
  

+ O(n) - при увеличении размера задачи в 2 раза, затраты времени возрастут тоже в два раза
+ O(n^2) - при увеличении размера задачи в 2 раза, затраты времени возрастут примерно в четыре раза
+ O(n\*log(n)) - при увеличении задачи в два раза, затраты времени возрастут в два раза, плюс некоторая прибавка, относительный вклад которой уменьшается с ростом n. При малых n может вносить очень большой вклад. O(n*log(n)) начинает расти как квадрат при малых n, но потом рост замедляется почти до линейного

### Примеры
+ O(n) — линейная сложность
Такой сложностью обладает, например, алгоритм поиска наибольшего элемента в не отсортированном массиве. Нам придётся пройтись по всем n элементам массива, чтобы понять, какой из них максимальный.

+ O(log n) — логарифмическая сложность
Простейший пример — бинарный поиск. Если массив отсортирован, мы можем проверить, есть ли в нём какое-то конкретное значение, методом деления пополам. Проверим средний элемент, если он больше искомого, то отбросим вторую половину массива — там его точно нет. Если же меньше, то наоборот — отбросим начальную половину. И так будем продолжать делить пополам, в итоге проверим log n элементов.

+ O(n^2) — квадратичная сложность
Такую сложность имеет, например, алгоритм сортировки вставками. В канонической реализации он представляет из себя два вложенных цикла: один, чтобы проходить по всему массиву, а второй, чтобы находить место очередному элементу в уже отсортированной части. Таким образом, количество операций будет зависеть от размера массива как n * n, т. е. n2.

### Сложности Java Collections API
+ ArrayList
    + лучшее O(1) - чтения из любого места (прямой доступ к памяти через массив) и вставка в конец;
    + худшее O(n) - вставка - будет тем хуже чем ближе к началу, т.к. придется копировать больше элементов. Например если вставлять в середину то будет O(n/2)
	
+ LinkedList
    + лучшее O(1) - все операции с первым/последним элементом;
    + худшее O(n) - все операции с элементом в середине, т.к. придется делать обход в цикле	
    
+ HashMap/HashSet
    + лучшее O(1) - все операции, если коллизий нет и вставка даже если коллизий много, т.к. всегда вставляется в начало корзины (связного списка);
    + худшее O(log(n)) - чтение, если у всех элементов одинаковый hashCode - начиная с Java 8 после достижения определенного порога размера корзины вместо связанных списков используются сбалансированные деревья (до этого было O(n))
  	
+ LinkedHashMap/LinkedHashSet
    + O(1)/O(log(n)) - так же как и у HashMap, но чуть больше памяти на хранение связей;
        
+ TreeMap/TreeSet
    + гарантировано не хуже O(log(n)) - все операции, т.к. красно-черное дерево гарантирует такую сложность в худшем случае
