# System design

Репозиторий с кучей теории и примерами по системного дизайну:
[Github System Design Primer](https://github.com/donnemartin/system-design-primer?tab=readme-ov-file#system-design-topics-start-here)

## Формализация функциональных требований
+ **Что делает система?**
  + Кратко сформулировать, какую задачу решает система и какую основную функцию выполняет. 
  + Определить тип нагрузки: транзакционная (booking, оплата), контентная (видео, медиа), коммуникационная (чаты, уведомления).
+ **Кто основные пользователи?**
  + Например, для системы видео хостинга основные пользователи - контент мэйкеры и зрители
+ **Какие основные сценарии будем прорабатывать?** 
  + Например, 2 сценария для видео хостинга - загрузка видео контент мэйкером и просмотр зрителем
+ **Границы проектирования**
  + **In scope**, например:  
    - API для загрузки и выдачи видео.  
    - Очереди для фоновой обработки.  
    - Система хранения оригиналов и производных файлов.
  + **Out of scope**, например:  
    - Аутентификация и авторизация (типовая).  
    - CDN и отдача контента (внешний сервис).  
    - Интерфейс пользователя, мобильные клиенты.
  + **Внешние зависимости** - указать все внешние системы, с которыми требуется взаимодействовать. Например:
    - CDN — внешний сервис, только интеграция через REST API.  
    - Платёжный шлюз — внешний, асинхронные callback-и.  
    - Email/SMS-провайдер — внешний, SLA не гарантируем.
+ **Остальное зависит от задачи**

---

## Формализация нефункциональных требований

+ **Сколько DAU (Daily Active Users)?** Либо сколько всего юзеров.
  + Доля DAU от всех юзеров обычно небольшая (5–20%), если это не популярная соцсеть типа VK (там около 50%).
  + При расчёте нагрузки отталкиваться стоит именно от DAU, а не общего количества зарегистрированных.

+ **Сколько запросов в секунду (RPS)?** Либо сколько запросов в день делает один юзер, если известно DAU.
  + Приблизительная оценка:  
    - < 1000 RPS — типовая нагрузка, справится любой Postgres/MySQL.  
    - 1k–10k RPS — потребуются реплики, кэш (Redis), возможно шардинг.  
    - \> 10k RPS — нужна распределённая архитектура (NoSQL, Kafka, очереди).
  + Доля запросов на чтение и запись?
    + Высокий процент чтений → кэширование (Redis, CDN, query cache).  
    + Высокий процент записей → очередь на запись, логическая репликация, write-optimized storage.  
    + Для read/write > 10:1 можно выделять отдельные read-реплики.
  + Пиковая нагрузка и шаблон трафика?
    + Есть ли выраженные пики (например, в 9:00, 18:00, сезонные акции)?  
    + Если да — нужно предусмотреть авто-масштабирование и warmup.
    + Если нет — можно использовать фиксированный кластер.

+ **Объем, тип и сроки хранения данных?**
  + Уточнить, нужно ли **долговременное хранение** и можно ли переносить архивные данные в DWH / холодное хранилище (S3, Glacier).
  + Если данные не удаляются, важно уточнить **скорость роста** (GB/день) и **retention policy**.
  + Для систем с большим количеством записей — уточнить, возможно ли **партиционирование** или **TTL**.
  + Тип данных
    + Структурированные (транзакции, заказы) → реляционные БД.  
    + Не структурированные (логи, события) → объектное или NoSQL-хранилище.
    + Нужно уточнить, есть ли **медиафайлы** — они всегда выносятся в отдельное object storage.

+ **Требуемая latency?**
  + Какая задержка допустима для ключевых операций (в мс или секундах)?
  + Нужно уточнить, возможен ли переход на **асинхронную модель**:  
    медленное обновление данных, тяжелые отчеты и т.п. можно выполнять через очередь или job-сервис.  
  + Типовые ориентиры:
    - < 100 мс — требуется in-memory хранение, WebSocket, локальные реплики.  
    - < 500 мс — нормальный SLA для большинства веб-приложений.  
    - \> 1 с — можно делать асинхронно.

+ **Требуемая availability?**
  + Если требуется HA (High Availability - выше 99%) — например, 99.9% (1.44 минуты простоя в день) — уточнить, **какие именно компоненты** должны быть высокодоступными.
  + Не всегда нужно обеспечивать HA для всех частей системы — возможно, только для критичных сервисов (оплата, API, авторизация).
  + При HA выше 99.9% потребуется резервирование в нескольких зонах или регионах (multi-AZ, multi-region).

+ **Требования к консистентности?**
  + Что важнее — **availability** или **consistency** при сбое?
  + Если важнее доступность → eventual consistency, репликация с лагом.  
  + Если важнее согласованность → одна master-реплика, синхронные записи, но выше latency.

+ **Географическое распределение пользователей?**
  + Если пользователи распределены по регионам — нужно CDN и, возможно, multi-region репликация.
  + Если регион один, можно ограничиться одной зоной доступности, что упростит архитектуру.
---

## Нотация моделирования C4

Это подход к визуализации архитектуры программного обеспечения, созданный Саймоном Брауном. Появился он в результате чтения лекций по архитектуре на курсе.

C4 включает четыре уровня представления:

1. [Context](../C4_example/C4_level1_Context.jpg): высокоуровневый взгляд на систему. Показывает приложения и пользователей, без технических деталей.
2. [Container](../C4_example/C4_level2_Container.jpg): углубляет представление системы, описывая основные части, или "контейнеры" (backend-приложение, веб-приложение, мобильного приложение, базы данных, файловая система), которые входят в состав системы. На этом уровне определены функции каждого контейнера, технологические решения по языкам программирования, протоколы взаимодействия.
3. [Component](../C4_example/C4_level3_Component.jpg): детализирует каждый контейнер, описывая его компоненты и их взаимодействие.
4. Code: наиболее детальный уровень, описывающий внутреннюю структуру каждого компонента. Часто используются UML-диаграммы для его описания. Не обязателен.

---

## Основные подходы в High load
+ Масштабирование (модульные монолиты, микросервисы) - вертикальное и горизонтальное. Горизонтальное также улучшает availability, т.к. резервирует ноды на случай падений
  + Вертикальное - за счет увеличения ресурсов
  + Горизонтальное - за счет реплицирования микросервисов и/или БД.
  + Репликации БД. Это запуск БД в режиме нескольких нод-копий, обычно 1 master нода - запись и чтение и N slave нод - только чтение (поддерживают из коробки MongoDB и многие NoSql, Postgres только с плагинами). Также есть подходы multi master (поддерживает Cassandra)
  + Шардирование БД. Можно применить при большом объеме данных. Это разбиение данных на шарды и распределение их между нодами, обычно с помощью хэш-функций по какому-то полю. Например, node = hash(column)%nodes, или с помощью consistent hashing. Однако запросы должны иметь фильтр по этому полю, иначе будет сканирование всех шардов и тогда это бессмысленно.
  + Партицирование таблицы по какому-то полю, например по дате. Это еще один подход при больших объемах таблиц (Postgres поддерживает из коробки). Т.е. таблица будет разбита на части по периодам дат. Запросы также должны иметь фильтр по этой дате, иначе будет сканирование всех партиций и тогда это бессмысленно.
+ Кэширование на разных уровнях (in-memory кэши, Redis, Memcached и др.)
+ CDN в качестве кэша для статических ресурсов (images, CSS, JavaScript files), video и других публичных, но не конфиденциальных данных. Это компании имеющие сервера, которые располагаются в разных географических регионах. 
+ Георезерв - датацентры размещенные в разных географических регионах. Это позволит уменьшить сетевые latency у местных жителей, а также помогает обеспечить HA (>= 99,9%).
+ Обычно для операционных задач выбираются БД типа OLTP (Postgres, MongoDB и др.), но они не очень хороши для тяжелых запросов чтения - тут нужны OLAP (Greenplum, ClickHouse и др.)
+ У каждого микросервиса должна быть своя БД
+ Микросервисы должны быть stateless, т.к. stateful приложение масштабировать очень сложно
+ Асинхронный обмен сообщениями между микросервисами обычно организуют через брокеры сообщений (Rabbit MQ, [Kafka](../kafka.md) и др.)
+ Распределенные транзакции между микросервисами с разными БД следует избегать, например стараться объединять транзакционную логику в один микросервис. Но если все таки требуется, то обеспечиваются такими паттернами как, например, [Сага](system_design.md#Сага)
+ Обеспечение observability
  + Логирование (ELK - elasticsearch, logstash и kibana)
  + Мониторинг и алертинг (grafana, prometheus и др.)
  + Трейсинг (zipkin, jaeger и др.)
+ Переход на реактивщину - позволит обеспечить работу сервиса при большом количестве запросов (используется асинхронный и неблокирующий код) - см. [Reactivity](../concurrency.md#Реактивность)
+ Эзотерические подходы с прогревом JIT для ускорения динамической компиляции

---

## Паттерны микросервисов

### Сага
+ Этот паттерн предназначен для управления распределенными транзакциями в микросервисной архитектуре.
+ При использовании паттерна каждая локальная транзакция обновляет данные в хранилище в рамках одного микросервиса и публикует событие или сообщение, которые, в свою очередь, запускают следующую локальную транзакцию и так далее.
+ Если локальная транзакция завершается с ошибкой, выполняется серия компенсирующих транзакций, которые отменяют изменения предыдущих транзакций.

Для координации транзакций существует два основных способа:

+ Хореография. Децентрализованная координация, при которой каждый микросервис прослушивает события/сообщения другого микросервиса и решает, следует предпринять действие или нет.
<img width="700" alt="saga-choreography.png" src="resources/saga-choreography.png">

+ Оркестровка. Централизованная координация, при которой отдельный компонент (оркестратор) сообщает микросервисам, какое действие необходимо выполнить далее.
<img width="600" alt="saga-orchestration.png" src="resources/saga-orchestration.png">

---

## Расчет размера данных на примере PostgreSQL
Numeric Types:
  + smallint: 2 bytes
  + integer (int): 4 bytes
  + bigint: 8 bytes
  + real: 4 bytes (single-precision floating-point)
  + double precision: 8 bytes (double-precision floating-point)
  + numeric (decimal): Variable. The actual storage requirement is 2 bytes for each group of 4 decimal digits, plus 3-8 bytes overhead. 

Character Types:
  + char(n): n chars (fixed-length, padded with spaces)
  + varchar(n): Variable, up to n chars plus overhead (variable-length, no padding)
  + text: Variable, plus overhead (variable-length, no explicit length limit)

Each letter takes 1-4 bytes, depending on the encoding (Latin - 1b, Russian - 2b, some Chinese characters or emojis - 4b)

Date/Time Types:
  + date: 4 bytes
  + time [without time zone]: 8 bytes
  + timestamp [without time zone]: 8 bytes
  + time with time zone: 8 bytes
  + timestamp with time zone: 8 bytes

Boolean Type:
  + boolean: 1 byte

Binary Data Types:
  + bytea: Variable, plus overhead

JSON Types:
  + json: Variable, plus overhead
  + jsonb: Variable, plus overhead (stored in a decomposed binary format, potentially larger than json for the same data but faster to query)

## Расчет мощностей на примерах

### 1. Backend (Java Spring Boot / Micronaut / Quarkus)

> Типичная нагрузка: REST API, CPU-bound, без тяжёлых вычислений  
> Средний ответ ~50–100 KB, latency ~100–200 мс  

| Цель | Общий RPS | Кол-во инстансов | vCPU **на инстанс** | RAM **на инстанс** | Примечания |
|------|------------|------------------|--------------------|-------------------|-------------|
| Малый сервис / staging | 100 | 1 | 1 | 1–2 GB | Прототип, без HA |
| Средняя прод. нагрузка | 1k | 3 | 2 | 2–4 GB | 300–400 RPS/инстанс, балансировщик |
| Средний high-load | 5k | 5–6 | 4 | 4–8 GB | ~800–1000 RPS/инстанс, тюнинг connection pool |
| Крупная система | 20k | 10–12 | 8 | 8–12 GB | latency <100 ms, async I/O |
| Очень крупная (B2C) | 100k | 25–30 | 8 | 12–16 GB | Stateless-only, горизонтальное масштабирование |

**Комментарий:**  
- Java сервис с типовой логикой (ORM, JSON, REST) обрабатывает **300–500 RPS на 1–2 vCPU**.  
- При внешних API или тяжёлой сериализации — дели оценку на 2.  
- JVM требует минимум **2×RAM от heap** (GC, metaspace, буферы).  

---

### 2. Redis (кэш и сессии)

> Типичные операции: get/set, объекты до 1 KB  

| Цель | Общий OPS | Кол-во нод | vCPU **на ноду** | RAM **на ноду** | Примечания |
|------|------------|-------------|------------------|----------------|-------------|
| Малый проект | 10k | 1 | 1 | 1 GB | Без репликации |
| Средний сервис | 100k | 2 | 2 | 2–4 GB | Master + Replica |
| Крупный проект | 500k | 3 | 4 | 8–16 GB | Cluster, key hashing |
| Сверхнагрузка | 1M+ | 5–6 | 8 | 16–32 GB | Redis Cluster, pipeline, async client |

**Комментарий:**  
- Redis — **однопоточный**, масштабирование горизонтальное.  
- Добавлять **25–50% RAM** к активным данным (LRU, TTL, репликация).  

---

### 3. PostgreSQL (OLTP, high read/write mix)

> Типовые операции: insert/update/read по индексу, до 1 KB payload  

| Цель | Общий запрос/сек | Кол-во нод | vCPU **на ноду** | RAM **на ноду** | Хранилище | Примечания |
|------|------------------|-------------|------------------|----------------|------------|-------------|
| Небольшой сервис | ≤500 | 1 | 2 | 4 GB | SSD 100 GB | Без реплики |
| Средняя нагрузка | 1–3k | 2 | 4 | 8–16 GB | SSD 500 GB | Master + Replica |
| High-load | 5–10k | 2 | 8 | 32–64 GB | NVMe 1 TB+ | Partitioning, connection pooling |
| Очень крупная | 20k+ | 3–4 | 16–24 | 64–128 GB | NVMe 2–4 TB | Шардинг (Citus, PGXL) |

**Комментарий:**  
- До **500 RPS/ядро** при грамотных индексах и пуле соединений.  
- RAM должна покрывать **активный working set (индексы + горячие данные)** ×3.  
- SSD IOPS: ~10k/vCPU для стабильной latency.  

---

### 4. MongoDB / Cassandra (NoSQL, write-heavy)

| Цель | Общий RPS | Кол-во нод | vCPU **на ноду** | RAM **на ноду** | Примечания |
|------|------------|-------------|------------------|----------------|-------------|
| Малый кластер | ≤2k | 3 | 2 | 4 GB | Replication factor 3 |
| Средний проект | 10k | 5 | 4 | 8 GB | 2–3 TB данных |
| Большой проект | 50k | 8 | 8 | 16–32 GB | GC tuning, compaction |
| Massive scale | 100k+ | 12+ | 8–16 | 32 GB | Async write, QUORUM consistency |

**Комментарий:**  
- ~10k RPS на 4 vCPU ноду.  
- Mongo: RAM ≈ 20–30% от горячих данных.  
- Cassandra: ориентирована на записи, не любит маленькие партиции.  

---

### 5. Kafka (event streaming)

| Цель | Сообщений/сек | Кол-во брокеров | vCPU **на брокер** | RAM **на брокер** | Диск | Примечания |
|------|----------------|-----------------|-------------------|------------------|------|-------------|
| Малый поток | 10k | 3 | 2 | 4 GB | SSD 200 GB | 3 brokers + zookeeper |
| Средний | 100k | 3–5 | 4 | 8 GB | SSD 500 GB | replication factor=2 |
| High-load | 1M | 6–8 | 8 | 16 GB | NVMe 1–2 TB | 8 partitions/topic |
| Massive scale | 5M+ | 10+ | 8–16 | 32 GB | NVMe 4 TB | Cluster + MirrorMaker |

**Комментарий:**  
- Kafka ограничена **диском и сетью**, не CPU.  
- 1 брокер ≈ 100 MB/s (при SSD).  
- 20–30% RAM на page cache.  

---

### 6. Пример комплексной системы

> Веб-приложение для бронирования (высокая конкуренция на запись)

| Компонент | Общий RPS | Инстансы / Ноды | CPU / RAM **на инстанс** | Примечания |
|------------|------------|----------------|----------------|-------------|
| API Gateway (NGINX) | 5k | 2 | 2C / 2GB | TLS termination |
| Java backend | 5k | 5 | 4C / 4GB | ~1000 RPS/инстанс |
| Redis cache | 200k ops | 2 | 2C / 4GB | cache + sessions |
| PostgreSQL | 3k | 2 (master+replica) | 8C / 32GB | SSD 1TB |
| Kafka (events) | 50k msg | 3 | 4C / 8GB | buffering |
| Monitoring / Logging | n/a | 2 | 2C / 4GB | Elastic/Prometheus |

**Итого:**  
- Суммарно: ~40 vCPU и ~100 GB RAM.  
- SSD: ~2 TB (с запасом ×1.5 на WAL и репликацию).  
- Сеть: ~1–2 Gbps пропускной способности.

